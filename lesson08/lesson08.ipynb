{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 8. GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для начала пример с вебинара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pics = np.concatenate([x_train, x_test])\n",
    "all_pics = all_pics.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pics = np.reshape(all_pics, (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(all_pics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=1024).batch(64).prefetch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 74,625\n",
      "Trainable params: 74,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              809088    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 1)         12545     \n",
      "=================================================================\n",
      "Total params: 1,608,449\n",
      "Trainable params: 1,608,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        # строим размер входного вектора 7x7x128 map\n",
    "        layers.Dense(7 * 7 * 128),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((7, 7, 128)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "        # берем случайный пример из скрытого пространства\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Строим по нему фейковое изображение\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # собрали с реальным в текзор\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # задаем метки 1 и 0 соответственно\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Добавляем шум !!!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # учим discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        #Выбрали случайный пример в скрытом пространстве\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # собрали метки реальных изображений\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Учим generator !\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1094/1094 [==============================] - 2259s 2s/step - d_loss: 0.4643 - g_loss: 1.4453\n",
      "Epoch 2/3\n",
      "1094/1094 [==============================] - 2341s 2s/step - d_loss: 0.6019 - g_loss: 1.1617\n",
      "Epoch 3/3\n",
      "1094/1094 [==============================] - 2413s 2s/step - d_loss: 0.7042 - g_loss: 0.7866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb436581790>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=3, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACHUlEQVR4nMVSTW8SURS9780nM8wwAwwfhbZiO9TG2gaTmmpw2UUTl/4CExcu3Ltwoxv/hXFhdONKTeNGF6aJUVNsiERbq0AsDqXUAgPDFGbmuaCA9Q94tueee++55yIYAgFiGCqVXcw+yJcbAABAw5jlk8rs/ZmDljedH9YPQVPx+bVr1FFJSN360AIAADwiiaQ/DL2+vfVtYuOw86+SolfSKWOHzpit6Sc7p5SIZblj8kO6cW+18yJAn1Ky+vwUvVSJVWaKvfSdUKPeGG+7cPFqQjPq/t31ibWW2pTk0sgKUm8qTC61G/p6LnbpOPwuqS1PvnQBgAYEGJXY/lzV8An7ihb4sl3Vi28IAACNgHjYn/AUOh32iyr13o3ovrN3r5gAgAkggrKcKMtBRu62tyNcgr2uPTuZSQDsDUvtih2uJUT9BWUr7cVd0Rz67BuFGumYJtu2RJ0VTDAeO6MjONaKzktTQh+7Zl09HyziR4to6DOYqU0i2/EFbdWOY650ufa5gok38JlIh7VoWSJtvyX9VvgLFabOAhq0xbHeJ1lbEoJyKMA4pcOKuPe05qFBW/L9FbM+d1DebOp0/jCQoWw74AIBwACYcJ08fm40FsyCxff292pHm0ULYQAEwLny8hkrF+ULP2nB9pxIL1ltuX1EAAHyebPd5qqd++V6g/x8mY/9YZ68wzMZ/Bb1KOckeUwAyPhFAP31MP8VfwA+dcmdQTq2QgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACWElEQVR4nFWSXUiTURjHn3M2t3fv6+b2bn4tZtuapYS9U0GLsESjZt2Y1ZUYgQRW99FN0E3dJUh4IREEddFNC0MrZWIXfaBBDBdq5lZrs6Yz9/W6j3fbebuY5fHcPHB+/P/POf/nAQAAwIzWrAcAAGGtSNaHTwB1lI0DEwNYjZWHiUxIMIJK1xgAAGQkbAyOCvenfeA95Ba9AiXEfP94bDnYQ5YmsdpYfDyvoG0xy3E2I+Mau1sGxlbt1BBtS9K58jUE4oOVyfePUqn5hyW40xrYju7j2zNne54/5UeUIMu0L5hsjoOnDRjgluDd/cROzfD19kYPk+7sZa4Csse2dnsCFE61WmuaNPhyw9vPoInGaKXq4jmNKm455khcW1cirnvUSCmdbZGQX3ehWv+sSccX+yQnrQwnW8PVld8d5OSb3o/pfrxwnYLit0s6kCtyoboq0vakfvA1FQLa5ypD2eBqwSL/2t/ZPMY7KKjVm1aSfxS1LFroZRUz9g+rFFQZEw0izsXjkZoJtNEW6ldRUOdX5e1afZbJa0zLk15+JEfB6HAtiYaWkuVNiVe/r7QEzGoKdr/kfki81byZLt6oSL7zhiUKGrJrmp855YZaW/uJFLqQYNgNAc2lh9ZNSYnLZKOB7Kalzz2H5P/zZMe1Lzi/YbZzkf3SccRT99V1J/MPYuy86QGAuC/IMmemE8J5y70VnNqB+q6q+Rbb1iKeJRmFQkJsZWybEeVStu3NYRKAKTFGAAoFkEURoCiXlIqjt0Xr9oGQ2+3fszulB2GZdbTnfT5p72L9Ba2E4v+Xdko7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACMklEQVR4nH2SPUwTYRzG3/e993rXu/boN23kNP0yUtBaJCEpqaYxKH5iwubg4qaTCYs7syZGE0cSGRgkxhiZGuJgF7RQtcYSBTkBSz+htL07ru2dQ4kpsfWZ/nl+z3/7AdA98D/snxBn2uYIHYWWAeLwIhmsHYWeO7dsrcYwMm0BuJ3pH0DzjZgAm4Q22fNN1NqhY0mJ77v9qAL5CVaZU1ufDJBo8op87fnN3XPHBneyEUzMBEJbEgYAuKZO51hpPFbkK+5mWcrxG30FKhE+mEcQouu3iwLpSsfLJitBAyUtZlY/+MzJLQSxrX/AzcghR3WdRzaxusJmjNpJ7/JH6eoUOn/3YSbsyr0VmwflYqmklUgLiNTiryf7HU1cig6JyUvHUwuBPGUv6ySctfNpTul7MZZRsRrHJm/h1dmwvDjqQDWq4pRp3y+BXXvm+A2Nw8yFKF/Y0+3oZeumvpEKWkghSL9M1PyzyEC4ZoaXn76vezXGSIvAImnNy/ub1REfV8N58+Mn86cC2WSllzbqvbJq9eA1tqgkhzYJRNu1ExNfp1V/+LOd4nt1dlat+wqzP9+tfFewWCSrC9lo/pE6eD/nDC3tGccpJpVQdLtsE6HVN/c+CevmoDNP1YQfDXF7ceNLuoHUUqwOgHkOEBHOY/COMQByBAKU3spRJNJBAADjAD0MMJgMFDy0ih4l/8rlhjR3MQhJguqgH9RotQHUrtKRAADUDRKd69aegBDhzoOu+QNUK88YqC4QFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"generated_img_0_2.png\"))\n",
    "display(Image(\"generated_img_1_2.png\"))\n",
    "display(Image(\"generated_img_2_2.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Какие-то огрызки получились вроде ))\n",
    "# Три эпохи - училось почти два часа.. ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
